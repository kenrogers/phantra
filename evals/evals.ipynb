{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Please enter your OpenAI API key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "First document preview: okay whiz we're talking about reasoning in latent space today is that the same as test time compute yeah that's right nice nice okay and we've got two big ideas to cover that are aimed at scaling the ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# Load transcript from video\n",
    "video_url = \"https://www.youtube.com/watch?v=BaTjJJsz0rY\"\n",
    "loader = YoutubeLoader.from_youtube_url(video_url)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} document(s)\")\n",
    "print(f\"First document preview: {docs[0].page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenny/code/ai/terrapin/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]                                           \n",
      "Generating Scenarios: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]\n",
      "Generating Samples: 100%|██████████| 10/10 [00:04<00:00,  2.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the key concepts related to GPT in th...</td>\n",
       "      <td>[okay whiz we're talking about reasoning in la...</td>\n",
       "      <td>In the context of reasoning in continuous late...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the scaling hypothesis in the context ...</td>\n",
       "      <td>[kind of doing this compression okay we're tak...</td>\n",
       "      <td>The scaling hypothesis suggests that going tok...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How did artificial neural networks evolve in t...</td>\n",
       "      <td>[like when you when you when you're trying to ...</td>\n",
       "      <td>Artificial neural networks began with the mult...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As an AI Research Scientist, how do you forese...</td>\n",
       "      <td>[let's let's you know these breakthroughs that...</td>\n",
       "      <td>By 2025, advancements in reasoning techniques ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the role of the Transformer in reasoni...</td>\n",
       "      <td>[hidden state of the llm as a representation o...</td>\n",
       "      <td>The Transformer plays a crucial role in reason...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the implications of reasoning in late...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nhidden state of the llm as a repre...</td>\n",
       "      <td>The implications of reasoning in latent space ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the implications of using latent spac...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nhidden state of the llm as a repre...</td>\n",
       "      <td>The implications of using latent space reasoni...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are the key differences between the recur...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nright where the green shared recur...</td>\n",
       "      <td>The key differences between the recurrent dept...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do the concepts of reasoning in latent spa...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nlike when you when you when you're...</td>\n",
       "      <td>The concepts of reasoning in latent space and ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the recurrent depth approach relate t...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nright where the green shared recur...</td>\n",
       "      <td>The recurrent depth approach, while not direct...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What are the key concepts related to GPT in th...   \n",
       "1  What is the scaling hypothesis in the context ...   \n",
       "2  How did artificial neural networks evolve in t...   \n",
       "3  As an AI Research Scientist, how do you forese...   \n",
       "4  What is the role of the Transformer in reasoni...   \n",
       "5  What are the implications of reasoning in late...   \n",
       "6  What are the implications of using latent spac...   \n",
       "7  What are the key differences between the recur...   \n",
       "8  How do the concepts of reasoning in latent spa...   \n",
       "9  How does the recurrent depth approach relate t...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [okay whiz we're talking about reasoning in la...   \n",
       "1  [kind of doing this compression okay we're tak...   \n",
       "2  [like when you when you when you're trying to ...   \n",
       "3  [let's let's you know these breakthroughs that...   \n",
       "4  [hidden state of the llm as a representation o...   \n",
       "5  [<1-hop>\\n\\nhidden state of the llm as a repre...   \n",
       "6  [<1-hop>\\n\\nhidden state of the llm as a repre...   \n",
       "7  [<1-hop>\\n\\nright where the green shared recur...   \n",
       "8  [<1-hop>\\n\\nlike when you when you when you're...   \n",
       "9  [<1-hop>\\n\\nright where the green shared recur...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  In the context of reasoning in continuous late...   \n",
       "1  The scaling hypothesis suggests that going tok...   \n",
       "2  Artificial neural networks began with the mult...   \n",
       "3  By 2025, advancements in reasoning techniques ...   \n",
       "4  The Transformer plays a crucial role in reason...   \n",
       "5  The implications of reasoning in latent space ...   \n",
       "6  The implications of using latent space reasoni...   \n",
       "7  The key differences between the recurrent dept...   \n",
       "8  The concepts of reasoning in latent space and ...   \n",
       "9  The recurrent depth approach, while not direct...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_specific_query_synthesizer  \n",
       "7  multi_hop_specific_query_synthesizer  \n",
       "8  multi_hop_specific_query_synthesizer  \n",
       "9  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"latent_space_youtube\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"latent_space_youtube\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "  retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Question: {question}\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Provide a clear, insightful answer using only the provided context.\n",
    "        If you cannot answer from the context, say \"Insufficient context to answer.\"\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "  messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
    "  response = llm.invoke(messages)\n",
    "  return {\"response\" : response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class State(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document]\n",
    "  response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_row in dataset:\n",
    "  response = graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the key concepts related to GPT in th...</td>\n",
       "      <td>[it's kind of funny in a logical way if you lo...</td>\n",
       "      <td>[okay whiz we're talking about reasoning in la...</td>\n",
       "      <td>The key concepts related to GPT in the context...</td>\n",
       "      <td>In the context of reasoning in continuous late...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the scaling hypothesis in the context ...</td>\n",
       "      <td>[that we might have there are many different s...</td>\n",
       "      <td>[kind of doing this compression okay we're tak...</td>\n",
       "      <td>The scaling hypothesis in the context of token...</td>\n",
       "      <td>The scaling hypothesis suggests that going tok...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How did artificial neural networks evolve in t...</td>\n",
       "      <td>[perceptron artificial neural networks and sin...</td>\n",
       "      <td>[like when you when you when you're trying to ...</td>\n",
       "      <td>Artificial neural networks have evolved signif...</td>\n",
       "      <td>Artificial neural networks began with the mult...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As an AI Research Scientist, how do you forese...</td>\n",
       "      <td>[the next step in the evolution of what we've ...</td>\n",
       "      <td>[let's let's you know these breakthroughs that...</td>\n",
       "      <td>By 2025, advancements in reasoning techniques ...</td>\n",
       "      <td>By 2025, advancements in reasoning techniques ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the role of the Transformer in reasoni...</td>\n",
       "      <td>[really is doing at a calculation level at tes...</td>\n",
       "      <td>[hidden state of the llm as a representation o...</td>\n",
       "      <td>The role of the Transformer in reasoning model...</td>\n",
       "      <td>The Transformer plays a crucial role in reason...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the implications of reasoning in late...</td>\n",
       "      <td>[okay whiz we're talking about reasoning in la...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nhidden state of the llm as a repre...</td>\n",
       "      <td>Reasoning in latent space presents significant...</td>\n",
       "      <td>The implications of reasoning in latent space ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the implications of using latent spac...</td>\n",
       "      <td>[it's kind of funny in a logical way if you lo...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nhidden state of the llm as a repre...</td>\n",
       "      <td>The implications of using latent space reasoni...</td>\n",
       "      <td>The implications of using latent space reasoni...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are the key differences between the recur...</td>\n",
       "      <td>[thanks whiz all right guys we are gonna rock ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nright where the green shared recur...</td>\n",
       "      <td>The key differences between the recurrent dept...</td>\n",
       "      <td>The key differences between the recurrent dept...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do the concepts of reasoning in latent spa...</td>\n",
       "      <td>[really is doing at a calculation level at tes...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nlike when you when you when you're...</td>\n",
       "      <td>The concepts of reasoning in latent space and ...</td>\n",
       "      <td>The concepts of reasoning in latent space and ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the recurrent depth approach relate t...</td>\n",
       "      <td>[it's pretty dope not just because a bunch of ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nright where the green shared recur...</td>\n",
       "      <td>The recurrent depth approach relates to Meta A...</td>\n",
       "      <td>The recurrent depth approach, while not direct...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What are the key concepts related to GPT in th...   \n",
       "1  What is the scaling hypothesis in the context ...   \n",
       "2  How did artificial neural networks evolve in t...   \n",
       "3  As an AI Research Scientist, how do you forese...   \n",
       "4  What is the role of the Transformer in reasoni...   \n",
       "5  What are the implications of reasoning in late...   \n",
       "6  What are the implications of using latent spac...   \n",
       "7  What are the key differences between the recur...   \n",
       "8  How do the concepts of reasoning in latent spa...   \n",
       "9  How does the recurrent depth approach relate t...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [it's kind of funny in a logical way if you lo...   \n",
       "1  [that we might have there are many different s...   \n",
       "2  [perceptron artificial neural networks and sin...   \n",
       "3  [the next step in the evolution of what we've ...   \n",
       "4  [really is doing at a calculation level at tes...   \n",
       "5  [okay whiz we're talking about reasoning in la...   \n",
       "6  [it's kind of funny in a logical way if you lo...   \n",
       "7  [thanks whiz all right guys we are gonna rock ...   \n",
       "8  [really is doing at a calculation level at tes...   \n",
       "9  [it's pretty dope not just because a bunch of ...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [okay whiz we're talking about reasoning in la...   \n",
       "1  [kind of doing this compression okay we're tak...   \n",
       "2  [like when you when you when you're trying to ...   \n",
       "3  [let's let's you know these breakthroughs that...   \n",
       "4  [hidden state of the llm as a representation o...   \n",
       "5  [<1-hop>\\n\\nhidden state of the llm as a repre...   \n",
       "6  [<1-hop>\\n\\nhidden state of the llm as a repre...   \n",
       "7  [<1-hop>\\n\\nright where the green shared recur...   \n",
       "8  [<1-hop>\\n\\nlike when you when you when you're...   \n",
       "9  [<1-hop>\\n\\nright where the green shared recur...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The key concepts related to GPT in the context...   \n",
       "1  The scaling hypothesis in the context of token...   \n",
       "2  Artificial neural networks have evolved signif...   \n",
       "3  By 2025, advancements in reasoning techniques ...   \n",
       "4  The role of the Transformer in reasoning model...   \n",
       "5  Reasoning in latent space presents significant...   \n",
       "6  The implications of using latent space reasoni...   \n",
       "7  The key differences between the recurrent dept...   \n",
       "8  The concepts of reasoning in latent space and ...   \n",
       "9  The recurrent depth approach relates to Meta A...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  In the context of reasoning in continuous late...   \n",
       "1  The scaling hypothesis suggests that going tok...   \n",
       "2  Artificial neural networks began with the mult...   \n",
       "3  By 2025, advancements in reasoning techniques ...   \n",
       "4  The Transformer plays a crucial role in reason...   \n",
       "5  The implications of reasoning in latent space ...   \n",
       "6  The implications of using latent space reasoni...   \n",
       "7  The key differences between the recurrent dept...   \n",
       "8  The concepts of reasoning in latent space and ...   \n",
       "9  The recurrent depth approach, while not direct...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_specific_query_synthesizer  \n",
       "7  multi_hop_specific_query_synthesizer  \n",
       "8  multi_hop_specific_query_synthesizer  \n",
       "9  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 60/60 [05:20<00:00,  5.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.8750, 'faithfulness': 0.8227, 'factual_correctness': 0.4340, 'answer_relevancy': 0.9860, 'context_entity_recall': 0.2497, 'noise_sensitivity_relevant': 0.3337}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextRecall, ContextPrecision, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), ContextPrecision(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id_set = set()\n",
    "\n",
    "for document in split_documents:\n",
    "  id = str(uuid.uuid4())\n",
    "  while id in id_set:\n",
    "    id = uuid.uuid4()\n",
    "  id_set.add(id)\n",
    "  document.metadata[\"id\"] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split_documents = split_documents[:len(split_documents) - 24]\n",
    "val_split_documents = split_documents[len(split_documents) - 24:102-12]\n",
    "test_split_documents = split_documents[102-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Given the following context, you must generate questions based on only the provided context.\n",
    "\n",
    "You are to generate {n_questions} questions which should be provided in the following format:\n",
    "\n",
    "1. QUESTION #1\n",
    "2. QUESTION #2\n",
    "...\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_chain = qa_prompt_template | qa_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "async def create_questions(documents, n_questions):\n",
    "  questions = {}\n",
    "  relevant_docs = {}\n",
    "\n",
    "  # Generate questions for each document\n",
    "  for doc in tqdm.tqdm(documents, desc=\"Processing documents\"):\n",
    "    doc_id = doc.metadata[\"id\"]\n",
    "\n",
    "    # Generate n questions for this document\n",
    "    for _ in range(n_questions):\n",
    "      # Generate a question using the chain\n",
    "      response = await question_generation_chain.ainvoke({\"context\": doc.page_content, \"n_questions\": n_questions})\n",
    "      question = response.content\n",
    "\n",
    "      # Generate unique ID for this question\n",
    "      question_id = str(uuid.uuid4())\n",
    "\n",
    "      # Store question and relevant doc mapping\n",
    "      questions[question_id] = question\n",
    "      relevant_docs[question_id] = [doc_id]\n",
    "\n",
    "  return questions, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 42/42 [01:15<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 24/24 [00:42<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
    "\n",
    "train_dataset = {\n",
    "    \"questions\" : training_questions,\n",
    "    \"relevant_contexts\" : training_relevant_contexts,\n",
    "    \"corpus\" : training_corpus\n",
    "}\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
    "\n",
    "val_dataset = {\n",
    "    \"questions\" : val_questions,\n",
    "    \"relevant_contexts\" : val_relevant_contexts,\n",
    "    \"corpus\" : val_corpus\n",
    "}\n",
    "\n",
    "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
    "\n",
    "test_dataset = {\n",
    "    \"questions\" : test_questions,\n",
    "    \"relevant_contexts\" : test_relevant_contexts,\n",
    "    \"corpus\" : train_corpus\n",
    "}\n",
    "\n",
    "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_dataset['corpus']\n",
    "queries = train_dataset['questions']\n",
    "relevant_docs = train_dataset['relevant_contexts']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    doc_id = relevant_docs[query_id][0]\n",
    "    text = corpus[doc_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    examples, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "corpus = val_dataset['corpus']\n",
    "queries = val_dataset['questions']\n",
    "relevant_docs = val_dataset['relevant_contexts']\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/wfomp2mq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x3abcd8f50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    }
   ],
   "source": [
    "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='finetuned_arctic_ft',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=50\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
