{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation for YouTube Insights\n",
    "\n",
    "This notebook generates synthetic test data from YouTube transcripts using RAGAS to help evaluate and improve our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "First document preview: okay whiz we're talking about reasoning in latent space today is that the same as test time compute yeah that's right nice nice okay and we've got two big ideas to cover that are aimed at scaling the ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# Load transcript from video\n",
    "video_url = \"https://www.youtube.com/watch?v=BaTjJJsz0rY\"\n",
    "loader = YoutubeLoader.from_youtube_url(video_url)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} document(s)\")\n",
    "print(f\"First document preview: {docs[0].page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]                                           \n",
      "Generating Scenarios: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it]\n",
      "Generating Samples: 100%|██████████| 10/10 [00:02<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the concept of Chain of Thought relat...</td>\n",
       "      <td>[okay whiz we're talking about reasoning in la...</td>\n",
       "      <td>The Chain of Thought, specifically the continu...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are embedding models and how do they rela...</td>\n",
       "      <td>[kind of doing this compression okay we're tak...</td>\n",
       "      <td>Embedding models are used to represent data po...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How did the multi-layer perceptron contribute ...</td>\n",
       "      <td>[like when you when you when you're trying to ...</td>\n",
       "      <td>The multi-layer perceptron played a significan...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What significant developments in AI are antici...</td>\n",
       "      <td>[let's let's you know these breakthroughs that...</td>\n",
       "      <td>In 2024, significant developments in AI are ex...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the significance of coconut in the con...</td>\n",
       "      <td>[hidden state of the llm as a representation o...</td>\n",
       "      <td>Coconut is significant because it represents a...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does the concept of latent space reasoning...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nlike that's what I thought was int...</td>\n",
       "      <td>The concept of latent space reasoning in GPT m...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the implications of reasoning in late...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nokay whiz we're talking about reas...</td>\n",
       "      <td>Reasoning in latent space has significant impl...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does the recurrent depth approach in the g...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nright where the green shared recur...</td>\n",
       "      <td>The recurrent depth approach in the gp2 archit...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the concept of test time compute rela...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nokay whiz we're talking about reas...</td>\n",
       "      <td>The concept of test time compute is closely re...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the key differences between the conti...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nokay whiz we're talking about reas...</td>\n",
       "      <td>The continuous Chain of Thought (coconut) and ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How does the concept of Chain of Thought relat...   \n",
       "1  What are embedding models and how do they rela...   \n",
       "2  How did the multi-layer perceptron contribute ...   \n",
       "3  What significant developments in AI are antici...   \n",
       "4  What is the significance of coconut in the con...   \n",
       "5  How does the concept of latent space reasoning...   \n",
       "6  What are the implications of reasoning in late...   \n",
       "7  How does the recurrent depth approach in the g...   \n",
       "8  How does the concept of test time compute rela...   \n",
       "9  What are the key differences between the conti...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [okay whiz we're talking about reasoning in la...   \n",
       "1  [kind of doing this compression okay we're tak...   \n",
       "2  [like when you when you when you're trying to ...   \n",
       "3  [let's let's you know these breakthroughs that...   \n",
       "4  [hidden state of the llm as a representation o...   \n",
       "5  [<1-hop>\\n\\nlike that's what I thought was int...   \n",
       "6  [<1-hop>\\n\\nokay whiz we're talking about reas...   \n",
       "7  [<1-hop>\\n\\nright where the green shared recur...   \n",
       "8  [<1-hop>\\n\\nokay whiz we're talking about reas...   \n",
       "9  [<1-hop>\\n\\nokay whiz we're talking about reas...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The Chain of Thought, specifically the continu...   \n",
       "1  Embedding models are used to represent data po...   \n",
       "2  The multi-layer perceptron played a significan...   \n",
       "3  In 2024, significant developments in AI are ex...   \n",
       "4  Coconut is significant because it represents a...   \n",
       "5  The concept of latent space reasoning in GPT m...   \n",
       "6  Reasoning in latent space has significant impl...   \n",
       "7  The recurrent depth approach in the gp2 archit...   \n",
       "8  The concept of test time compute is closely re...   \n",
       "9  The continuous Chain of Thought (coconut) and ...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_specific_query_synthesizer  \n",
       "7  multi_hop_specific_query_synthesizer  \n",
       "8  multi_hop_specific_query_synthesizer  \n",
       "9  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"Reasoning in Continuous Latent Space\"\n",
    "\n",
    "langsmith_dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Reasoning in Continuous Latent Space\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_row in dataset.to_pandas().iterrows():\n",
    "  client.create_example(\n",
    "      inputs={\n",
    "          \"question\": data_row[1][\"user_input\"]\n",
    "      },\n",
    "      outputs={\n",
    "          \"answer\": data_row[1][\"reference\"]\n",
    "      },\n",
    "      metadata={\n",
    "          \"context\": data_row[1][\"reference_contexts\"]\n",
    "      },\n",
    "      dataset_id=langsmith_dataset.id\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_documents = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "rag_documents = text_splitter.split_documents(rag_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=rag_documents,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"State of AI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Question: {question}\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Provide a clear, insightful answer using only the provided context.\n",
    "        If you cannot answer from the context, say \"Insufficient context to answer.\"\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Latent space refers to a conceptual representation used in models, particularly in machine learning and AI, where data is transformed into a mathematical space that captures its essential features. It's distinct from natural language or token space and is often described in terms of embedding space, which consists of numerical representations of data. In the context of Transformer architectures, like those used in language models, latent space is where information is processed and reasoned before being converted back into human-readable language. This involves hidden states and scaling of the model, where the representation of reasoning about data occurs. Essentially, latent space serves as an intermediary space that facilitates computation and reasoning without directly operating in the space of words or language.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"question\" : \"What is latent space?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langsmith Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
    "\n",
    "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"helpfulness\": (\n",
    "                \"Is this submission helpful to the user,\"\n",
    "                \" taking into account the correct reference answer?\"\n",
    "            )\n",
    "        },\n",
    "        \"llm\" : eval_llm\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"output\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "dope_or_nope_evaluator = LangChainStringEvaluator(\n",
    "    \"criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"dopeness\": \"Is this submission dope, lit, or cool?\",\n",
    "        },\n",
    "        \"llm\" : eval_llm\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
