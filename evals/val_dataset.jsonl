{"questions": {"bf7e47b9-b0f8-444f-af8a-08143a9b11b3": "1. What is the purpose of the demo being introduced in the context?\n2. What does the coconut implementation from meta include, and what does it not include?", "da24353b-3e27-40c4-ae55-bf9d05216c12": "1. What is the purpose of the demo being introduced in the context?\n2. What does the coconut implementation from meta include, and what does it not include?", "00c69c2d-ae11-43ca-ac22-d58fc430a470": "1. What is the main idea behind the coconut pie's approach to modifying the Transformer architecture?\n2. How does the processing of latent tokens differ from the processing of actual manifested tokens in the described method?", "f1a39a30-cd4b-4ff2-a47e-c28c15215451": "1. What is the main idea behind the coconut pie's approach to modifying the Transformer architecture?\n2. How does the processing of latent tokens differ from that of manifested tokens in the context described?", "119f4133-88c6-4615-b073-288f5b8d42e0": "1. What is the main idea behind the process described in the context regarding token generation?\n2. How does the approach mentioned in the context differ from traditional methods of handling tokens in a language model?", "1cd95d5d-b7cb-46d6-90b1-f831be9c59d6": "1. What is the main idea behind the process described in the context regarding token generation?\n2. How does the approach mentioned in the context differ from traditional methods of handling tokens in a language model?", "7f38d39a-d84e-4365-8ce5-b720b55d7264": "1. What is the purpose of the \"little loopy\" mentioned in the context?\n2. How does the process of exiting into token space differ from the initial steps described?", "f22fc88d-ee0e-4a51-a929-8a595463f662": "1. What is the purpose of the \"little loopy\" mentioned in the context?\n2. How does the process of exiting into token space differ from the initial operations in embedding space?", "dbcf69dd-d3d1-425e-8aee-8b38803fa9d0": "1. What is the purpose of training the model by eliminating the steps in the reasoning process?\n2. How does the model incorporate the beginning and end of thinking tokens during the training process?", "cfb25137-7d79-4f12-8227-3abc9523bcb9": "1. What is the process described for training the model in relation to the Chain of Thought steps?\n2. How does the model handle the reasoning process in the latent space according to the context?", "5a88c5be-b2b5-4737-a0bb-094ce8a6e852": "1. What is the minimum number of blocks that the model mentioned in the context works with?\n2. How does the approach described in the context differ from the recurrent depth model outlined by Greg?", "eaa31600-615a-426c-b70c-a3ef1afee744": "1. What is the minimum number of blocks that the model mentioned in the context works with?  \n2. How does the approach described in the context differ from the recurrent depth model outlined by Greg?", "eeda18dc-d986-45a7-aaee-c562ec4716e1": "1. How many total layers does the default recurrent depth model have?\n2. What is the recommended minimum number of recurrent blocks mentioned in the paper?", "37f4b89c-eb0c-4412-be87-661529c93f85": "1. How many total layers does the default recurrent depth model have?\n2. What is the recommended minimum number of recurrent blocks mentioned in the paper?", "e15df327-d3de-467d-86eb-3a253ea493a7": "1. What is the purpose of using B float 16 when loading the model for Cole LM Auto tokenizer?  \n2. Why is it important to have a GPU when running the model outside of a collab notebook?", "d7c43d63-8983-41db-bde0-02b59f30e15b": "1. What is the purpose of using B float 16 when loading the model for Cole LM Auto tokenizer?  \n2. Why is it important to have a GPU when running the model outside of a collab notebook?", "f64801c1-3bc0-472b-996a-2d73216ad174": "1. What is the purpose of the adapter mentioned in the context between the Prelude and the recurrent blocks?  \n2. How many layers are there in the recurrent block described in the context?", "2ee74b0b-22c5-4e97-8a3f-cc1b04567dc6": "1. What is the purpose of the adapter mentioned in the context between the Prelude and the recurrent blocks?  \n2. How many layers are there in the recurrent block described in the context?", "421fc5db-9035-46c0-9008-d33d8010d5fd": "1. What is the purpose of the adapter mentioned in the context?  \n2. How many Clips did Natalia sell in total in April and May according to the example provided?", "173fa149-afbc-4569-b2bc-c62238bc2535": "1. What is the purpose of the adapter mentioned in the context between Prelude and recurrent blocks?  \n2. How does the system handle the decoding of information from the recurrent blocks to the token world?", "a7d43ed8-7d5e-463b-aa01-46937b43f7c8": "1. How many Clips did Natalia sell in April if she sold half as many in May?\n2. What is the correct total number of Clips sold by Natalia in April and May after adjusting the number of recurrent blocks?", "3fe9de08-1677-4524-864d-d4254112feff": "1. How many Clips did Natalia sell in April if she sold half as many in May?\n2. What is the correct total number of Clips sold by Natalia in April and May combined?", "7a53fc48-47b7-44b3-9b0e-a459bc428b8b": "1. What was the correct answer obtained after the first calculation mentioned in the context?\n2. How many steps were initially taken before determining that the output was not satisfactory?", "93d9371a-8dda-4941-8a10-dfcc3a9d03f9": "1. What was the correct answer obtained after the first calculation mentioned in the context?\n2. How many steps were initially taken before determining that the output was not satisfactory?", "b1ec3d95-6350-479a-9510-915cf37de341": "1. What is the significance of using more recurrent blocks for difficult questions in the classification process?\n2. How does the number of blocks used in inference affect the accuracy of the answers provided?", "0b6d2e12-5cef-4b91-841d-6fcf721b759b": "1. What is the significance of using more recurrent blocks for difficult questions in the classification process?\n2. How does the number of blocks used in inference affect the accuracy of the answers provided?", "ea85a528-1dc3-4299-972c-10557afabfa5": "1. What is the main idea discussed regarding the use of tokens for reasoning in the context of LLMs?  \n2. How might going tokenless impact the efficiency of tasks according to the discussion?", "aefbd4a1-581b-4a27-bfe7-a572b3704e53": "1. What is the main idea discussed regarding the use of tokens in reasoning during the session?  \n2. How might going tokenless improve efficiency in certain tasks, according to the context provided?", "abe805ad-82a9-4745-8067-c44e41d012e4": "1. What are the potential benefits of going tokenless in terms of efficiency and response quality?\n2. How might the process of reasoning in latent space affect the clarity of the final answer when verbalized?", "1b952668-6829-446a-ad1c-6cf11134a9f4": "1. What are the potential benefits of going tokenless in terms of efficiency and response time?\n2. How might the process of reasoning in latent space affect the clarity of the final answer when verbalized?", "67267fa2-dc53-4033-b348-690939e81e71": "1. How does the efficacy of a solution depend on the strategy used in the context of decoding from the laden space?  \n2. What are the potential drawbacks of trying to retrieve information from the laden space in terms of computational efficiency?", "3fc2d4dd-2e22-429a-b472-a3895fc51bd7": "1. How does the efficacy of a solution depend on the strategy used in the context of decoding from the laden space?  \n2. What are the potential drawbacks of trying to retrieve information from the laden space in terms of computational efficiency?", "9980896a-10e5-4255-bcd8-72557902276e": "1. What is the significance of learning all the languages of the world in relation to understanding a rich token space?\n2. Have any experiments been conducted on the effects of removing positional encoding in the Prelude block?", "30a50fdd-e240-4350-bd35-8c37f46f7059": "1. What is the significance of learning all the languages of the world in relation to understanding a rich token space?\n2. Have any experiments been conducted on the effects of removing positional encoding in the Prelude block?", "c4eca694-7948-4bd3-aac8-5de835a324c0": "1. What is the significance of positional information in the context of language processing as mentioned in the discussion?\n2. How might different embeddings impact the current depth approach in research, according to the speaker's perspective?", "f3bee5d3-4048-47b7-84ab-187d97975b14": "1. What is the significance of positional information in the context of language processing as mentioned in the discussion?\n2. How might different embeddings impact the current depth approach in research, according to the speaker's perspective?", "da0c1c54-66a9-4bb4-877d-4e0e159514a0": "1. What is the significance of using a Transformer model to generate embeddings in the context discussed?\n2. How do the embeddings relate to the internals of the Transformers and tokens through the dense network?", "d97658a7-799c-432e-8428-a76a8158e64d": "1. What is the significance of using a Transformer model to generate embeddings in the context discussed?  \n2. How do the embeddings relate to the internals of the Transformers and tokens through the dense network?", "fd042c99-7748-42c1-960b-9dabf04cbae3": "1. What is the relationship between hidden states, embedding space, and latent space as discussed in the context?  \n2. Why is there a distinction made between staying in embedding space and going to token world?", "b41ac880-13e2-4095-b6f2-020a91837161": "1. What is the relationship between hidden states, embedding space, and latent space as discussed in the context?  \n2. Why is there a distinction made between staying in embedding space and going to token world?", "bffd4a0d-4353-48ea-be8d-30e4333c29cd": "1. What reasoning for transparency is being discussed in relation to Oliver Mills' comment in the YouTube chat?  \n2. What challenges are mentioned regarding decoding back into tokens from the latent space?", "e56ce9c1-aa38-48bb-9b1c-9ffe064553e1": "1. What reasoning for transparency is being discussed in relation to Oliver Mills' comment in the YouTube chat?  \n2. What challenges are mentioned regarding decoding back into tokens from the latent space?", "7dbb06fc-83c5-497c-acbd-5df67824af35": "1. What is the purpose of the upcoming lrm event mentioned in the context?  \n2. How can individuals join the community discussed in the context?", "5a644ddb-1163-480a-a86e-b5fce271e970": "1. What is the purpose of the upcoming lrm event mentioned in the context?  \n2. How can individuals join the community discussed in the context?", "f141b4b3-96cf-4e6d-8fdf-a2e4744cee11": "1. What is the focus of the upcoming cohort of the AI engineering boot camp?  \n2. What is the significance of the tool \"cursor\" mentioned in the context?", "90832ba7-5a16-4e94-9cc4-c092ed0c3e29": "1. What is the focus of the upcoming cohort of the AI engineering boot camp?  \n2. What is the significance of the tool \"cursor\" mentioned in the context?", "3d7b1cb0-ac39-4d13-b3a5-6874a864dccb": "1. What is the main focus of the upcoming event mentioned in the context?\n2. How can participants provide feedback on today's event?", "565e7d91-b7dd-45cd-a17e-930b3fb337d3": "1. What is the main focus of the upcoming event mentioned in the context?\n2. How can participants provide feedback on today's event?"}, "relevant_contexts": {"bf7e47b9-b0f8-444f-af8a-08143a9b11b3": ["abe772a0-4758-4506-8543-52471226b4e5"], "da24353b-3e27-40c4-ae55-bf9d05216c12": ["abe772a0-4758-4506-8543-52471226b4e5"], "00c69c2d-ae11-43ca-ac22-d58fc430a470": ["21816443-4bab-42d0-8772-340a3d7a0e79"], "f1a39a30-cd4b-4ff2-a47e-c28c15215451": ["21816443-4bab-42d0-8772-340a3d7a0e79"], "119f4133-88c6-4615-b073-288f5b8d42e0": ["48d83591-d538-4240-9ebd-464f3ef9de23"], "1cd95d5d-b7cb-46d6-90b1-f831be9c59d6": ["48d83591-d538-4240-9ebd-464f3ef9de23"], "7f38d39a-d84e-4365-8ce5-b720b55d7264": ["35edc68b-affb-4a7a-940c-9b6cf4b5810c"], "f22fc88d-ee0e-4a51-a929-8a595463f662": ["35edc68b-affb-4a7a-940c-9b6cf4b5810c"], "dbcf69dd-d3d1-425e-8aee-8b38803fa9d0": ["cdebc1d6-e162-45de-be61-3c6f48e8104f"], "cfb25137-7d79-4f12-8227-3abc9523bcb9": ["cdebc1d6-e162-45de-be61-3c6f48e8104f"], "5a88c5be-b2b5-4737-a0bb-094ce8a6e852": ["904f4a7f-8baf-406c-87a6-0256d243c8e9"], "eaa31600-615a-426c-b70c-a3ef1afee744": ["904f4a7f-8baf-406c-87a6-0256d243c8e9"], "eeda18dc-d986-45a7-aaee-c562ec4716e1": ["cc21c08e-359f-47c1-a176-49161a52c887"], "37f4b89c-eb0c-4412-be87-661529c93f85": ["cc21c08e-359f-47c1-a176-49161a52c887"], "e15df327-d3de-467d-86eb-3a253ea493a7": ["c10f9263-482b-42b3-b667-c4dc7c913895"], "d7c43d63-8983-41db-bde0-02b59f30e15b": ["c10f9263-482b-42b3-b667-c4dc7c913895"], "f64801c1-3bc0-472b-996a-2d73216ad174": ["8493040c-0a8f-4bf1-8d61-b68d54f83a03"], "2ee74b0b-22c5-4e97-8a3f-cc1b04567dc6": ["8493040c-0a8f-4bf1-8d61-b68d54f83a03"], "421fc5db-9035-46c0-9008-d33d8010d5fd": ["184859cb-26d0-49dc-a65d-d547691f4ec1"], "173fa149-afbc-4569-b2bc-c62238bc2535": ["184859cb-26d0-49dc-a65d-d547691f4ec1"], "a7d43ed8-7d5e-463b-aa01-46937b43f7c8": ["2b9bcc3c-18f0-4f11-995b-454680a2249c"], "3fe9de08-1677-4524-864d-d4254112feff": ["2b9bcc3c-18f0-4f11-995b-454680a2249c"], "7a53fc48-47b7-44b3-9b0e-a459bc428b8b": ["0e08bf24-0ce5-4a70-b84e-225ccaec97b2"], "93d9371a-8dda-4941-8a10-dfcc3a9d03f9": ["0e08bf24-0ce5-4a70-b84e-225ccaec97b2"], "b1ec3d95-6350-479a-9510-915cf37de341": ["46fac113-c454-486a-bb4e-2a5deeead0ec"], "0b6d2e12-5cef-4b91-841d-6fcf721b759b": ["46fac113-c454-486a-bb4e-2a5deeead0ec"], "ea85a528-1dc3-4299-972c-10557afabfa5": ["7448a45a-86bf-4071-84de-f0c9e127c421"], "aefbd4a1-581b-4a27-bfe7-a572b3704e53": ["7448a45a-86bf-4071-84de-f0c9e127c421"], "abe805ad-82a9-4745-8067-c44e41d012e4": ["e4907336-266d-4ca0-a025-16123c8e5df8"], "1b952668-6829-446a-ad1c-6cf11134a9f4": ["e4907336-266d-4ca0-a025-16123c8e5df8"], "67267fa2-dc53-4033-b348-690939e81e71": ["f9fb99c0-d47f-4d2f-8278-3986e0d5932d"], "3fc2d4dd-2e22-429a-b472-a3895fc51bd7": ["f9fb99c0-d47f-4d2f-8278-3986e0d5932d"], "9980896a-10e5-4255-bcd8-72557902276e": ["f759a1a6-ca7c-408a-9bd0-794116811459"], "30a50fdd-e240-4350-bd35-8c37f46f7059": ["f759a1a6-ca7c-408a-9bd0-794116811459"], "c4eca694-7948-4bd3-aac8-5de835a324c0": ["a6d8ad02-8f41-4fbd-b6dd-6cd3b0824776"], "f3bee5d3-4048-47b7-84ab-187d97975b14": ["a6d8ad02-8f41-4fbd-b6dd-6cd3b0824776"], "da0c1c54-66a9-4bb4-877d-4e0e159514a0": ["9237c33d-cfa7-4df9-a290-6f12c0c471cd"], "d97658a7-799c-432e-8428-a76a8158e64d": ["9237c33d-cfa7-4df9-a290-6f12c0c471cd"], "fd042c99-7748-42c1-960b-9dabf04cbae3": ["793cafc1-d769-4187-8e81-5f81072c19ab"], "b41ac880-13e2-4095-b6f2-020a91837161": ["793cafc1-d769-4187-8e81-5f81072c19ab"], "bffd4a0d-4353-48ea-be8d-30e4333c29cd": ["98216453-819a-4250-ad02-76f3d3df5a8b"], "e56ce9c1-aa38-48bb-9b1c-9ffe064553e1": ["98216453-819a-4250-ad02-76f3d3df5a8b"], "7dbb06fc-83c5-497c-acbd-5df67824af35": ["f4b99a11-98b6-47bc-bd2d-ab6b9bd33795"], "5a644ddb-1163-480a-a86e-b5fce271e970": ["f4b99a11-98b6-47bc-bd2d-ab6b9bd33795"], "f141b4b3-96cf-4e6d-8fdf-a2e4744cee11": ["2642d771-b4c2-41cb-8a69-0966e3a6ae2d"], "90832ba7-5a16-4e94-9cc4-c092ed0c3e29": ["2642d771-b4c2-41cb-8a69-0966e3a6ae2d"], "3d7b1cb0-ac39-4d13-b3a5-6874a864dccb": ["cc37cebe-8e80-4c09-b0f5-ce897f9bba8c"], "565e7d91-b7dd-45cd-a17e-930b3fb337d3": ["cc37cebe-8e80-4c09-b0f5-ce897f9bba8c"]}, "corpus": {"abe772a0-4758-4506-8543-52471226b4e5": "can concretely say we know what to do but we know what to do with the code so I want to go ahead and just quickly introduce this to you uh whiz we're going to do a quick demo here and we're going to use the official implementation from meta on coconut as well as look at Rec current depth uh whiz I'm gonna go ahead and hand it off to you to take lat and space reasoning for spin for everybody yes okay so let's take it for a spin we've got uh a repo to look at and then we've got a model to look at so coconut in includes all the training code and everything but does not include the actual uh model so we're going to look at the recurrent depth model since they Lish the uh the weights a way to go them uh so we already have seen these kinds of diagrams that's great getting started is easy uh if you want to train your own coconut which I I don't necessarily uh suggest that you do uh for sure uh but basically what we have is we have our coconut. pie our coconut. Pie has this idea of uh you", "21816443-4bab-42d0-8772-340a3d7a0e79": "if you want to train your own coconut which I I don't necessarily uh suggest that you do uh for sure uh but basically what we have is we have our coconut. pie our coconut. Pie has this idea of uh you know basically just hacking the normal uh the the normal kind of the usual I would say uh Transformer architecture uh so we have this idea that we can you know do the uh the kind of latent tokens right which are not actually manifested tokens but we still need to keep track of them they're still part of a sequence etc etc uh then of course we have our actual processing of this where we're going to do this little uh pass in the max number of Laten steps right you can see all we're doing really is just looping around in this pass right so we're not we're not doing much else than that right so you can see like we don't there's no real uh there's no real like big modification here we just take the end of the thing instead of doing more tokens right we do Laten space thinking we just Loop", "48d83591-d538-4240-9ebd-464f3ef9de23": "so you can see like we don't there's no real uh there's no real like big modification here we just take the end of the thing instead of doing more tokens right we do Laten space thinking we just Loop through that Laten space and then eventually we want to go ahead and we want to send this to token world right where we're going to use our our base causal LM so this is our normal llm and the idea is because when we create the next token right when we're actually generating our our next token uh we we don't have a and this is most apparent in in our generate stuff right we don't have the idea like the tokens are in their embedded form right inside the model so we don't have have to decode them if we already have them decoded and this is kind of like the brilliant the brilliant idea of coconut right again we just have this little loopy and all we're doing is we're saying hey don't exit in into token space yet and then when we finally do need to exit we can just put a special token and", "35edc68b-affb-4a7a-940c-9b6cf4b5810c": "right again we just have this little loopy and all we're doing is we're saying hey don't exit in into token space yet and then when we finally do need to exit we can just put a special token and then say okay now we're going to go back to token world right we're going to start decoding into the token output and that's really it like you'll notice this is a 200 you 70 or whatever line file that's the that's the big change right and the only thing that's that's really different is that we have this little mini loop where we're gonna say hey we're gonna think around in that in that in that in that uh embedding space as opposed to going to token world so how did they get the ability to do this right that's like the big question so how they got the ability to do this is most clearly exp expressed through this diagram which is basically saying how did we how did we do this how did we get here well we took a uh you know a query W which had Chain of Thought steps and then eventually an answer", "cdebc1d6-e162-45de-be61-3c6f48e8104f": "through this diagram which is basically saying how did we how did we do this how did we get here well we took a uh you know a query W which had Chain of Thought steps and then eventually an answer and essentially what we did is we trained the model by slowly eliminating the steps right so we we slowly eliminated the actual steps so you'll you'll notice that first we're going to train the model uh in order to give it these special beginning of thinking end of thinking tokens then we're going to replace step one one and we're going to shove it in the hidden space right in between our beginning and end of thinking tokens then we're going to train it on two steps missing and then we're going to train it on all steps missing right this is the idea is we're slowly tucking that reasoning process into the Len space but the key here is that each of these thoughts is another still another path right we're still able to think insteps in Laten space we're just not resolving to token World until", "904f4a7f-8baf-406c-87a6-0256d243c8e9": "into the Len space but the key here is that each of these thoughts is another still another path right we're still able to think insteps in Laten space we're just not resolving to token World until the end but this is like the this is the big idea of coconut right this is slightly different than the big idea from the recurrent death death model as Greg has already outlined where we're actually going to dynamically create blocks of uh of recurrent depth uh or recurrent uh uh you know thinking at time of inference this is much more uh immediately scalable by humans right because there's a parameter now which is how many blocks do we want right and the the big thing about this model is that it works with the minimum number of blocks which is going to be four in this case so the default uh you know the default uh recurrent depth model is going to have you know eight uh for instance eight uh I'll go ahead and share the collab link definitely in the in the chat uh but it's going to have", "cc21c08e-359f-47c1-a176-49161a52c887": "you know the default uh recurrent depth model is going to have you know eight uh for instance eight uh I'll go ahead and share the collab link definitely in the in the chat uh but it's going to have eight total layers right because there's there's going to be a number of layers per recurrent block block as well as a number of layers in our Prelude and Koda blocks but we can just decide AR to make it arbitrarily large now in the paper they say it should be at least four recurrent blocks right plus the Prelude and Koda and it should all it should more likely be closer to 16 right but you can go to 64 and potentially see some increase so let's see how this thing actually goes uh let's see how it how it works it's just a hugging face model so we'll zoom in a little bit more uh we we load it the same way we always do auto model for Cole LM Auto tokenizer nice and easy we're loading this thing at B float 16 just to keep it small uh we download it it's great we're going to choose our device", "c10f9263-482b-42b3-b667-c4dc7c913895": "same way we always do auto model for Cole LM Auto tokenizer nice and easy we're loading this thing at B float 16 just to keep it small uh we download it it's great we're going to choose our device appropriately uh you're going to be hard pressed and run this outside of a collab notebook with a GPU instance so make sure you have a GPU and then we can set it into eval mode we're going to set this config as provided by the uh uh by the the actual model card for the recurrent depth model uh but you'll see we fairly short max length right we have these stop strings this is strings where we stop the generation right uh we have use cache so this uses KV cache right in order to to do do good at its job uh but what this means is that we have to say use cach there's a lot of instructions on how we can exploit the KV cache in the model card which is very fun and then uh of course we're going to have a bunch of other generic hyperparameters for for generation and then we can see our actual model", "8493040c-0a8f-4bf1-8d61-b68d54f83a03": "can exploit the KV cache in the model card which is very fun and then uh of course we're going to have a bunch of other generic hyperparameters for for generation and then we can see our actual model right so we have our Transformer model classic we have our uh text embeddings classic right and then we have our Prelude block then we have some adapter that adapts between the Prelude and the rest of our blocks we have our core block which is going to be it's called I'm not joking the sandwich block right uh and that's going to basically be all these are the four layers that exist inside our recurrent block so there's always going to be one of these recurrent blocks by default right with four layers I get this for eight layers total uh but this is the idea is here that we have this uh adapter between our our Prelude and our Co our recurrent blocks then we have our n uh you know recurrent blocks in this case it's just one with its with its four layers very cool and then we have our", "184859cb-26d0-49dc-a65d-d547691f4ec1": "adapter between our our Prelude and our Co our recurrent blocks then we have our n uh you know recurrent blocks in this case it's just one with its with its four layers very cool and then we have our finally Aroda arota is just uh again the thing that gets us to text world right to token world uh because we have to decode what these recurrent blocks have been thinking about what they've been stirring on uh so we're going to look at an example of how to actually use the thing it does support templates so chat templates just like most models wasn't explicitly trained for this task so you're going to see some quirkiness and funest and weirdness but it is still possible to use it and we're just going to ask some GSM 8K questions you know Natalia sold Clips to 48 of her friends in April and then sold half as many Clips in May how many Clips did Natalia sell all together in April and May classic math question and you can see that we send this through and that this step right this nums step", "2b9bcc3c-18f0-4f11-995b-454680a2249c": "sold half as many Clips in May how many Clips did Natalia sell all together in April and May classic math question and you can see that we send this through and that this step right this nums step or this nums steps parameter is the thing that says how many how many recurrent blocks do we want right in this case we're gon to say four uh because it recommends using at least four and that's the reason why uh and you can see that we get this response uh that she sold 96 Clips because she sold 48 plus 48 hey that's not correct right that's wrong in fact so uh maybe if we juice up the number of steps it'll be less wrong so let's go to eight recurrent blocks instead of four recur block and again the only thing we're changing is the number of blocks that's going on right and we get here and then we get the correct answer of 72 hey nice perfect and then we do it again and we get again the correct answer so at 16 it's also correct that that would be assumed let's try another example so we have", "0e08bf24-0ce5-4a70-b84e-225ccaec97b2": "we get the correct answer of 72 hey nice perfect and then we do it again and we get again the correct answer so at 16 it's also correct that that would be assumed let's try another example so we have this next Ken created a care package sent to his brother who is away at boarding school can place box blah blah blah blah it's another it's another math question right okay let's see it so we get here again we're going to start at four steps and then we're going to see if it did it and we get the weight of the box of double the weight of the box of gummy worms so this is it just becomes nonsense right like this is this is this is not a good output uh four four steps was not enough the model produced garbage let's try eight steps all right eight steps now is is less garbage the final weight of the box of goodies is 12 pounds but it's wrong that's not correct uh you know so so eight steps won't do it what about 16 steps and again we're not reloading the model we're not we can do this every", "46fac113-c454-486a-bb4e-2a5deeead0ec": "the box of goodies is 12 pounds but it's wrong that's not correct uh you know so so eight steps won't do it what about 16 steps and again we're not reloading the model we're not we can do this every time we do inference every time we call inference we can change this number and it's going to use a certain number of blocks right so think about if you have a classifier that classifies how difficult the question is right you could say for very difficult questions use more recurrent blocks for very easy questions use less maybe don't use four because produces garbage but then we see in the 16 not only do we get the correct answer of 16 pounds but we also see the classic let's think through it stepbystep pattern emerge right so this is the idea we add more blocks we get more think we get more think we get more accurate uh such as the world of reasoning there you go I'm G to pass you guys back to Greg who can take us into some Q&A before I do please remember to like And subscribe and ring", "7448a45a-86bf-4071-84de-f0c9e127c421": "think we get more accurate uh such as the world of reasoning there you go I'm G to pass you guys back to Greg who can take us into some Q&A before I do please remember to like And subscribe and ring the bell notification we're live every Wednesday doing this we love doing it uh there you go all right back to Greg all right yeah let's go got the thinky thinky going and it actually worked what a beautiful thing uh all right let's go ahead and wrap up and take some conclusions away from the day before we get into QA here you know the big idea that hopefully you're walking away with today is that we don't need to use tokens directly for reasoning like we do but llms don't if we don't use tokens we get this less compression effect and coconut showed us this can be done very dope and recurrent depth tells us maybe it could even be more efficient especially if we start thinking about tasks that are really optimized for this kind of approach going tokenless might allow us to scale tokenless", "e4907336-266d-4ca0-a025-16123c8e5df8": "depth tells us maybe it could even be more efficient especially if we start thinking about tasks that are really optimized for this kind of approach going tokenless might allow us to scale tokenless we'll get into the pedantic picy details of that in lrm events to come I'm sure that is we could potentially use fewer tokens to get the same response in less time maybe we could even get better responses and get the trifecta okay I want to go ahead and go to QA now if you haven't put your questions into to the slid go ahead and throw them in there now whiz let's take a look at the QA we got some great stuff going today I want to kick it off with yon bores is it possible that after the process of reasoning in latent space And then trying to verbalize The Final Answer into human language it's unrecognizable I mean I think we kind of saw some of that just a minute ago in the demo yeah you can definitely do that uh it's hard uh so and your strategy is GNA depend on or the the uh efficacy of", "f9fb99c0-d47f-4d2f-8278-3986e0d5932d": "unrecognizable I mean I think we kind of saw some of that just a minute ago in the demo yeah you can definitely do that uh it's hard uh so and your strategy is GNA depend on or the the uh efficacy of that solution is going to depend on your strategy but you you can certainly try to get some of that information back out of the laden space remember that the the laden space uh uh thinking right still counts where we're generating tokens uh so it's going to be it's going to depend on the strategy that we're using how we're decoding are we decoding all of it etc etc but we we can definitely get back to token world uh though the process might be more computationally inefficient so it might not be desirable uh aside from areas where we need explainable explainability yeah yeah yeah yeah I also really just this kind of a sidebar I kind of really like this intuition of like if we're learning all the languages of the world then this Rich this very rich token space beyond what we can sort of", "f759a1a6-ca7c-408a-9bd0-794116811459": "really just this kind of a sidebar I kind of really like this intuition of like if we're learning all the languages of the world then this Rich this very rich token space beyond what we can sort of Imagine in our minds starts to make a lot more intuitive sense um uh okay so going to uh the next we got a a bit of a a meme question here the little memer dreamer action for those of you out there from Anonymous do these techniques change the fact that LMS are just stochastic parrots does nothing it does nothing to address that all right moving there you go Anonymous has there have there been experiments done yet with no positional encoding in the Prelude block I don't I don't imagine so uh we still care about sequence order so we still care about position uh in language so so mayhaps there's some there's some way where we don't care about uh the the positional information when we're when we're thinking around in in in Laden space but that experiment has not yet been run and my my", "a6d8ad02-8f41-4fbd-b6dd-6cd3b0824776": "there's some there's some way where we don't care about uh the the positional information when we're when we're thinking around in in in Laden space but that experiment has not yet been run and my my intuition would say that uh we we want to keep some positional information just because language has we do have to resolve back to language at some point and that should helpfully make maybe if indeed time is a flat circle it would work but I'm not I'm not sure uh okay so let's go to the next one here Anonymous any research on different embeddings and their impact on their current depth approach I would imagine this is something they've talked about in the research groups that are that are working on this but I mean this was sort of one of my questions too is like does it matter and of course it matters I mean what's your what's your take on this wi yeah it must matter uh I don't think we're we're we're that deep into it yet but we're we are like they are using a Transformer model to", "9237c33d-cfa7-4df9-a290-6f12c0c471cd": "it matters I mean what's your what's your take on this wi yeah it must matter uh I don't think we're we're we're that deep into it yet but we're we are like they are using a Transformer model to generate the embeddings I'd imagine that we'd see a lot of uh fun uh work based on this exploring different embedding approaches Beyond just you know this this work um it should have some impact what impact is uh remains to be SE nice okay does the collab run on T4 GPU 399 uh you'll definitely be able to run up to eight 16 might be a little hairy okay all right and then a final question from the slido today from Anonymous the embeddings link between internals of the Transformers and tokens through the dense Network the embeddings link between internals of the Transformer and tokens through the dense Network I would think thinking in latent space relies on hidden States I think there's a disambiguation of language thing here um I want to clarify my understanding and and maybe maybe you can sort", "793cafc1-d769-4187-8e81-5f81072c19ab": "Network I would think thinking in latent space relies on hidden States I think there's a disambiguation of language thing here um I want to clarify my understanding and and maybe maybe you can sort of triple down on it with like when we talk about hidden States when we talk about embedding space when we talk about latent space when we talk about math and compute space we're talking about the same thing it's just not language space um I'm not sure if that addresses the question or if you have anything to add here uh no not nothing to add I mean if I understand the question correctly uh we don't care about going to token World until we do we do stay in in in embedding space until we exit in either case uh so uh I might be misunderstanding the question but we we yeah we don't need to go back until we do go back okay and then I think related to what you just said we've got Oliver Mills in the in the YouTube chat here um can you pull out some of the reasoning for transparency uh realize", "98216453-819a-4250-ad02-76f3d3df5a8b": "until we do go back okay and then I think related to what you just said we've got Oliver Mills in the in the YouTube chat here um can you pull out some of the reasoning for transparency uh realize the cost but value um I'm not sure what the the last part says but the first part I think the answer is yes yeah it must be true that we can squeeze some juice from this I don't know how productive that would be uh exactly but but we must be able to remember those that each step that's occurring in the latent space right we would basically just decode back into tokens so that it might not make sense because it didn't have to make sense in token world right so uh the we can get maybe a summary or or or something like that but to go directly to we'd have to build in an additional process and that additional process would kind of be a a spoiler you know a uh uh holding us back computation yeah yeah yeah yeah right right we don't want to put that extra burden on it otherwise why do we go do this", "f4b99a11-98b6-47bc-bd2d-ab6b9bd33795": "process would kind of be a a spoiler you know a uh uh holding us back computation yeah yeah yeah yeah right right we don't want to put that extra burden on it otherwise why do we go do this in the first place okay whiz thank you for all of your amazing insights today we look forward to the next lrm event in the series and it's time to go ahead and wrap up shout out to some of our great community members in the chat today including Sahar AI by AI black karant great to see you guys out there and uh and shout out to Cil and Wayne as well I'd love to see the reg the regulars coming in if you haven't joined our community and you aren't regular yet it's a great time to jump in and join us in the Discord I am live every Monday with a growing group of people telling you what they're building shipping and sharing this week including some of the folks I just mentioned and we are there to hold each other accountable to really get after it and Achieve our dreams at the llm edge if that's of", "2642d771-b4c2-41cb-8a69-0966e3a6ae2d": "shipping and sharing this week including some of the folks I just mentioned and we are there to hold each other accountable to really get after it and Achieve our dreams at the llm edge if that's of interest to you come join us next week for free if you want to accelerate your llm application development consider joining the upcoming cohort of the AI engineering boot camp this will be cohort 6 and we're excited to incorporate more reasoning we're excited to incorporate more Vision language models and multimodality we're excited to incorporate everything that is really kind of production ready at the llm edge including what we're talking about next week which is cursor probably one of the most hype AI tools out there if you haven't dug into cursor yet you got to get into it we're going to be doing some Vibe coding with whiz this is like coding in natural language it's going to be a ton of fun and it's going to be a real blast of an event so join us next week Live on YouTube same time", "cc37cebe-8e80-4c09-b0f5-ce897f9bba8c": "be doing some Vibe coding with whiz this is like coding in natural language it's going to be a ton of fun and it's going to be a real blast of an event so join us next week Live on YouTube same time same place you know where to find us to talk cursor it's a guide for AI Engineers or aspiring AI Engineers or people trying to lead a engineering teams and with that we'd love to hear any feedback you have on today's event either directly in the chat on Discord or reach out to any of us personally and that's wrap for coconut for recurrent depth for lrms I look for more lrm series stuff coming we've got some other series we're cooking up so keep building shipping and sharing in the meantime and you know that we will do the same have a great week everybody see you next Wednesday [Music]"}}